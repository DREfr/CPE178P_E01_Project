import mindspore.dataset as ds
import mindspore.dataset.vision.c_transforms as CV
from mindspore import dtype as mstype

# Paths for your dataset folders
train_data_path = 'rocks_train'
val_data_path = 'rocks_val'

# Only the selected 12 rock classes
rock_classes = ["Basalt", "Chert", "Coal", "Gneiss", "Granite", 
                "Limestone", "Marble", "Obsidian", "Pumice", 
                "Sandstone", "Slate", "Travertine"]

# Automatically assign class indexes
class_indexing = {name: idx for idx, name in enumerate(rock_classes)}

def create_dataset(data_path, batch_size=32, training=True):
    """Define the rock classification dataset with only 12 classes."""

    data_set = ds.ImageFolderDataset(data_path, num_parallel_workers=8, shuffle=True,
                                     class_indexing=class_indexing)

    # Image normalization setup
    image_size = 224
    mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]
    std = [0.229 * 255, 0.224 * 255, 0.225 * 255]

    # Data augmentation
    if training:
        trans = [
            CV.RandomCropDecodeResize(image_size, scale=(0.08, 1.0), ratio=(0.75, 1.333)),
            CV.RandomHorizontalFlip(prob=0.5),
            CV.Normalize(mean=mean, std=std),
            CV.HWC2CHW()
        ]
    else:
        trans = [
            CV.Decode(),
            CV.Resize(256),
            CV.CenterCrop(image_size),
            CV.Normalize(mean=mean, std=std),
            CV.HWC2CHW()
        ]

    data_set = data_set.map(operations=trans, input_columns="image", num_parallel_workers=8)
    data_set = data_set.batch(batch_size, drop_remainder=True)

    return data_set

# Load datasets
dataset_train = create_dataset(train_data_path, training=True)
dataset_val = create_dataset(val_data_path, training=False)

Step 2	Visualize the dataset.
The return value of the training dataset loaded from the create_dataset API is a dictionary. You can use the create_dict_iterator API to create a data iterator and use next to iteratively access the dataset. Here, batch_size is set to 18. Therefore, you can use next to obtain 18 images and label data at a time

import matplotlib.pyplot as plt
import numpy as np

# Map labels to class names (only the 12 classes you are using)
rock_classes = ["Basalt", "Chert", "Coal", "Gneiss", "Granite", 
                "Limestone", "Marble", "Obsidian", "Pumice", 
                "Sandstone", "Slate", "Travertine"]

# Fetch one batch of data
data = next(dataset_train.create_dict_iterator())
images, labels = data["image"], data["label"]

print("Tensor of images:", images.shape)
print("Labels:", labels.asnumpy())

# Mean and std used in normalization (undo for display)
mean = np.array([0.485, 0.456, 0.406])
std = np.array([0.229, 0.224, 0.225])

# Display a sample grid (first 18 images or less)
num_to_show = min(18, images.shape[0])
plt.figure(figsize=(15, 8))

for i in range(num_to_show):
    img = images[i].asnumpy()
    img = np.transpose(img, (1, 2, 0))  # CHW -> HWC
    img = std * img + mean  # denormalize
    img = np.clip(img, 0, 1)  # clip to [0,1] for matplotlib
    label_idx = int(labels[i].asnumpy())

    plt.subplot(3, 6, i + 1)
    plt.imshow(img)
    plt.title(rock_classes[label_idx], fontsize=8)
    plt.axis("off")

plt.tight_layout()
plt.show()
