import mindspore.dataset as ds
import mindspore.dataset.vision.c_transforms as CV
from mindspore import dtype as mstype

# Paths for your dataset folders
train_data_path = 'rocks_train'
val_data_path = 'rocks_val'

# Only the selected 12 rock classes
rock_classes = ["Basalt", "Chert", "Coal", "Gneiss", "Granite", 
                "Limestone", "Marble", "Obsidian", "Pumice", 
                "Sandstone", "Slate", "Travertine"]

# Automatically assign class indexes
class_indexing = {name: idx for idx, name in enumerate(rock_classes)}

def create_dataset(data_path, batch_size=32, training=True):
    """Define the rock classification dataset with only 12 classes."""

    data_set = ds.ImageFolderDataset(data_path, num_parallel_workers=8, shuffle=True,
                                     class_indexing=class_indexing)

    # Image normalization setup
    image_size = 224
    mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]
    std = [0.229 * 255, 0.224 * 255, 0.225 * 255]

    # Data augmentation
    if training:
        trans = [
            CV.RandomCropDecodeResize(image_size, scale=(0.08, 1.0), ratio=(0.75, 1.333)),
            CV.RandomHorizontalFlip(prob=0.5),
            CV.Normalize(mean=mean, std=std),
            CV.HWC2CHW()
        ]
    else:
        trans = [
            CV.Decode(),
            CV.Resize(256),
            CV.CenterCrop(image_size),
            CV.Normalize(mean=mean, std=std),
            CV.HWC2CHW()
        ]

    data_set = data_set.map(operations=trans, input_columns="image", num_parallel_workers=8)
    data_set = data_set.batch(batch_size, drop_remainder=True)

    return data_set

# Load datasets
dataset_train = create_dataset(train_data_path, training=True)
dataset_val = create_dataset(val_data_path, training=False)
